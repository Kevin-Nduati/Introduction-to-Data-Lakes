{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config.get('AWS', 'ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = config.get('AWS', 'SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "config = pyspark.SparkConf()\n",
    "config.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "config.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "config.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "config.set(\"spark.hadoop.metrics.conf.file\", \"/home/kevin/spark/spark-3.3.1-bin-hadoop3/conf/metrics.properties\") \n",
    "config.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "config.set(\"spark.jars\", \"/home/kevin/spark/hadoop-aws-3.2.4.jar,/home/kevin/spark/aws-java-sdk-bundle-1.11.901.jar\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- staff_id: string (nullable = true)\n",
      " |-- rental_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .load(\"s3a://pagiladata1/payment.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2020-01-25 00:40:...|\n",
      "|     16051|        269|       1|       98|  0.99|2020-01-25 18:16:...|\n",
      "|     16052|        269|       2|      678|  6.99|2020-01-29 00:44:...|\n",
      "|     16053|        269|       2|      703|  0.99|2020-01-29 03:58:...|\n",
      "|     16054|        269|       1|      750|  4.99|2020-01-29 11:10:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- staff_id: string (nullable = true)\n",
      " |-- rental_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn('payment_date', F.to_timestamp('payment_date'))\n",
    "dfPayment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|month|\n",
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "|     16050|        269|       2|        7|  1.99|2020-01-25 00:40:...|    1|\n",
      "|     16051|        269|       1|       98|  0.99|2020-01-25 18:16:...|    1|\n",
      "|     16052|        269|       2|      678|  6.99|2020-01-29 00:44:...|    1|\n",
      "|     16053|        269|       2|      703|  0.99|2020-01-29 03:58:...|    1|\n",
      "|     16054|        269|       1|      750|  4.99|2020-01-29 11:10:...|    1|\n",
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPayment = dfPayment.withColumn('month', F.month('payment_date'))\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e2c05c7f250bfb303f80aa1691662391266cf7df9ebc5d1bd1356cd3eeb295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
