{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need Data Lakes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, many factors drove the evolution of the data warehouse:\n",
    "1. The abundance of unstructured data(text, xml, json, logs, sensor data, images, voice, etc)\n",
    "2. Unprecedented data volumes (social, IoT, machine-generated, etc..)\n",
    "3. The rise of big data technologies like HDFS, Spark, etc\n",
    "4. New types of data analysis gaining momentum e.g predictive analysis. recommender systems, graph analytics etc\n",
    "5. Emergence of new roles such as data scientists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we have unstructured data in the data warehouse\n",
    "* Might be possible in the ETL process. For instance, we might be able to distill some elements from json data and put it in a tabular format\n",
    "* But later, we might decide that we want to transform it differently, so deciding on a particular form of transformation is a strong commitment without enough knowledge. e.g we start by recording # of replies in a facebook of comments and then we are interested in the frequency of angry words.\n",
    "* Some data is hard to put in a tabular format like deep json structures\n",
    "* Some data like text/pdf documents could be stored as blobs of data in a relational database but totally useless unless processed to extract metrics.\n",
    "* The Hadoop File System (HDFS) made it possible to store perabytes of data on commodity hardware. Much lower cost per TB compared to MPP databases\n",
    "* Associated processing tools starting with MapReduce, Pig, Hive, Impala and Spark to name a few made it possible to process this data at scale on the same hardware used for storage.\n",
    "* It is possible to make data analysis without inserting into a predefined schema. One can load a CSV file and make a query without creating a table, inserting the data in the table. Similarly, one can process unstructured text. This approach is known as \"Schema on read\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Roles and Advanced Analytics\n",
    "* The data warehouse by design follows a very well-architected path to make a clean, consistent and performant model that business users can easily use to gain insights and make decisions\n",
    "* As data became an asset of highest value, a role like the data scientist began to emerge seeking value from data\n",
    "* The data scientist job is almost impossible conforming to a sinbgle rigid representation of data. She needs freedomn to represent data, join datasets together and retrieve new external data sources and more\n",
    "* The type of analytics such as e.g, machine learning, natural language processing need to access the raw data in forms totally different from a star schema.\n",
    "* The data lakes shares the goals of the data warehouse of supporting business insights beyond the day to day transactional data handling\n",
    "\n",
    "The data lake is a new form of a data warehouse that evolved to cope with:\n",
    "* The variety of data formats and structuring\n",
    "* The agile and ad hoc nature of dfata exploration activities needed by new roles such as data scientists\n",
    "* The wide spectrum data transformation needed by advanced analytics like machine learning, graph analytics and recommender systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Technology Effect on Data Warehousing\n",
    "* Once big data technology started gaining industrial grounds, ETL offloading for data warehouse was a clear choice\n",
    "* Same hardware for storage and processing. No need for a special ETL grid or additional storagfe for a staging area\n",
    "* Dimensional modeling with conformed dimensions or data marts for high/known-value data\n",
    "* Moreover, low cost per TB gave room for storing low/unknown value data previously not available in analytics\n",
    "\n",
    "* Traditionally, data in a database has been much easier to process than data in plain files. Big data tools in the hadoop ecosystem e.g Hive & Spark made it easy to work with a file as easy as it is to work with a database without creating a database and inserting the data into the database.\n",
    "* Schema on read: it is either inferred or specified and the data is not inserted into it, but upon read the data is checked against the specified schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
